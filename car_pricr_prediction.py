# -*- coding: utf-8 -*-
"""car_pricr_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OsSSwCPcPYdgFeyDfF2rXdm3ZpmM9UEW
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# importing the dataset
car_df = pd.read_csv("/content/car data.csv")

print(car_df.columns)

# display the first few rows of the dataset
car_df.head()

# display some basic information about the dataset
car_df.info()

# check for missing values
car_df.isnull().sum()

# check for duplicate values
car_df.duplicated().sum()

# drop the duplicate values
car_df.drop_duplicates(inplace=True)
car_df.duplicated().sum()

# Histogram

sns.set_style("whitegrid")

# plot histogram for numerical coulumns
car_df[['Selling_Price', 'Present_Price', 'Driven_kms']].hist(bins=20, figsize=(10, 7), edgecolor='black')
plt.suptitle('Histograms of Numerical Features', fontsize=16)
plt.show()

# Box plot to visualize outliers in numerical columns
plt.figure(figsize=(12, 6))

# Subplot for each numerical feature
plt.subplot(1, 3, 1)
sns.boxplot(y=car_df['Selling_Price'])
plt.title('Selling Price')

plt.subplot(1, 3, 2)
sns.boxplot(y=car_df['Present_Price'])
plt.title('Present Price')

plt.subplot(1, 3, 3)
sns.boxplot(y=car_df['Driven_kms'])
plt.title('Driven Kilometers')

plt.suptitle('Box Plots of Numerical Features', fontsize=16)
plt.show()

# Select numerical columns
numeric_columns = ['Year', 'Selling_Price', 'Present_Price', 'Driven_kms', 'Owner']

# numeric dataframe
numeric_df = car_df[numeric_columns]

# correlation matrix for numerical columns
correlation_matrix = numeric_df.corr()

# Create a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap (Numerical Columns)')
plt.show()

# Pair plot to visualize relationships between numerical features
sns.pairplot(car_df[['Selling_Price', 'Present_Price', 'Driven_kms']])
plt.suptitle('Pair Plot of Numerical Features', fontsize=16)
plt.show()

# Box plot for Selling Price across different Fuel Types
plt.figure(figsize=(8, 6))
sns.boxplot(x='Fuel_Type', y='Selling_Price', data=car_df)
plt.title('Selling Price Across Different Fuel Types', fontsize=16)
plt.xlabel('Fuel Type')
plt.ylabel('Selling Price')
plt.show()

# Box plot for Selling Price across different Transmission Types
plt.figure(figsize=(8, 6))
sns.boxplot(x='Transmission', y='Selling_Price', data=car_df)
plt.title('Selling Price Across Different Transmission Types', fontsize=16)
plt.xlabel('Transmission')
plt.ylabel('Selling Price')
plt.show()

# Distribution plot with KDE for Selling Price
plt.figure(figsize=(8, 6))
sns.distplot(car_df['Selling_Price'], kde=True, bins=30)
plt.title('Distribution of Selling Price', fontsize=16)
plt.xlabel('Selling Price')
plt.ylabel('Density')
plt.show()

# data cleaning and preprocessing
# handeling categorial variable

# One-hot encoding of categorical variables
car_df = pd.get_dummies(car_df, columns=['Fuel_Type', 'Selling_type', 'Transmission'], drop_first=True)

# Create a new feature 'Car_Age'
car_df['Car_Age'] = 2024 - car_df['Year']

# Drop the 'Year' and 'Car_Name' columns as they are no longer needed
car_df = car_df.drop(['Year', 'Car_Name'], axis=1)

# Separate features and target variable
X = car_df.drop('Selling_Price', axis=1)
y = car_df['Selling_Price']

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Linear Regression
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Predict on the test set
y_pred_lr = lr_model.predict(X_test)

# Evaluate the model
print("Linear Regression - R^2 Score:", r2_score(y_test, y_pred_lr))
print("Linear Regression - RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_lr)))
print('Mean squared eroor', mean_squared_error(y_test, y_pred_lr))

# Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict on the test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
print("Random Forest - R^2 Score:", r2_score(y_test, y_pred_rf))
print("Random Forest - RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_rf)))

